# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['text_dedup', 'text_dedup.utils']

package_data = \
{'': ['*']}

install_requires = \
['bitarray>=2.6.2,<3.0.0',
 'dask[bag]>=2023.1.0,<2024.0.0',
 'datasets>=2.4.0,<3.0.0',
 'numpy>=1.23.2,<2.0.0',
 'pybloom-live>=4.0.0,<5.0.0',
 'pyspark>=3.3.1,<4.0.0',
 'rich>=12.5.1,<13.0.0',
 'scipy==1.9.1',
 'tqdm>=4.64.1,<5.0.0',
 'xxhash>=3.0.0,<4.0.0']

setup_kwargs = {
    'name': 'text-dedup',
    'version': '0.3.1',
    'description': 'All-in-one text deduplication',
    'long_description': '<center><img src="./banner.png"/ style="background-color:white;"></center>\n\n![GitHub](https://img.shields.io/github/license/ChenghaoMou/text-dedup) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/cc66178e49d24908ac1fb2b2dbe4e5b3)](https://www.codacy.com/gh/ChenghaoMou/text-dedup/dashboard?utm_source=github.com&utm_medium=referral&utm_content=ChenghaoMou/text-dedup&utm_campaign=Badge_Grade) [![Codacy Badge](https://app.codacy.com/project/badge/Coverage/cc66178e49d24908ac1fb2b2dbe4e5b3)](https://www.codacy.com/gh/ChenghaoMou/text-dedup/dashboard?utm_source=github.com&utm_medium=referral&utm_content=ChenghaoMou/text-dedup&utm_campaign=Badge_Coverage)\n\n## Features\n\nThis repository contains a collection of text deduplication scripts that are ready to use, or modify based on your needs:\n\n- MinHash + MinHashLSH, including a spark implementation suitable for large (TB) datasets\n- 64 or 128 bit SimHash\n- SuffixArray Substring\n- Bloom Filter\n- Exact Hash\n\nI also have big plans for the future:\n\n- [ ] Memory benchmark for streaming processing\n- [ ] Inter-dataset deduplication\n- [ ] Rewrite suffix array in Python\n- [ ] A collections of other deduplication methods: SuperMinHash, ProbMinHash, TreeMinHash, BagMinHash, [Optimal Densification for Fast and Accurate Minwise Hashing](https://arxiv.org/abs/1703.04664), [Fast Similarity Sketching](https://arxiv.org/abs/1704.04370)\n- [ ] A collections of other deduplication methods used in other places: [CCNet](https://github.com/facebookresearch/cc_net/blob/main/cc_net/dedup.py).\n\nHowever, I do not intent to build a general purpose deduplication library, which was the goal of this repo early on. I will gradually retire the pypi package as well. The reason behind it is that each use-case can be wildly different and requires careful design and consideration. I sincerely encourage you to read the script first (they are relatively short) so you can understand what are at stake here when using it. You can use it to bootstrap your own script, or just use it as a reference.\n\n## Acknowledgements\n\nThis repository is inspired by the following projects, and is heavily influenced by lessons learned from my own participation in [BigScience (Apache 2.0)](https://github.com/bigscience-workshop) and [BigCode (Apache 2.0)](https://github.com/bigcode-project). There is a [blog post](https://publish.obsidian.md/chenghao/posts/20230220150602) about the journey. Feedbacks are welcome!\n\n- [Datasketch](https://github.com/ekzhu/datasketch) (MIT)\n- [simhash-py](https://github.com/seomoz/simhash-py/tree/master/simhash) and [simhash-cpp](https://github.com/seomoz/simhash-cpp) (MIT)\n- [Deduplicating Training Data Makes Language Models Better](https://github.com/google-research/deduplicate-text-datasets) (Apache 2.0)\n- [Gaoya](https://github.com/serega/gaoya) (MIT)\n\n## Quick Examples\n\n### PySpark with DataProc\n\nNot a lot of people have access to enough compute resources or the need to deduplicate TB-scale datasets, but if you do, this is a good example of how to use it with GCP DataProc.\n\n*MODIFY `text_dedup/minhash_spark.py` FOR YOUR OWN PROJECT AND DATASET FIRST!*\n\n```bash\nexport CLUSTER_NAME=chenghao-temp\nexport PROJECT_ID=xx\n\ngcloud dataproc clusters create $CLUSTER_NAME \\\n    --enable-component-gateway \\\n    --region us-central1 \\\n    --zone us-central1-a \\\n    --master-machine-type c2d-standard-16 \\\n    --master-boot-disk-size 500 \\\n    --num-workers 10 \\\n    --worker-machine-type c2d-standard-16 \\\n    --worker-boot-disk-size 500 \\\n    --image-version 2.0-debian10 \\\n    --project $PROJECT_ID\n\ngcloud dataproc jobs submit pyspark --cluster ${CLUSTER_NAME} \\\n    --region us-central1 \\\n    --jars gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\n    --driver-log-levels root=WARN \\\n    --properties="spark.executor.memory"="50g","spark.driver.memory"="8g","spark.executor.cores"="14" \\\n    minhash_spark.py -- \\\n    --table "huggingface-science-codeparrot.the_stack_java.java" \\\n    --output "gs://chenghao-data/dataproc_output/deduplicated"\n```\n\nFor reference, the script finished deduplicating 42 million rows in less than 40 minutes with above settings (160 cores, 640GB memory in total), while the python version would take around 10 hours with a 80-core machine with 1.8TB memory.\n\nIn the following part, we are going to deduplicate one dataset: `gl` subset of `oscar-corpus/OSCAR-2201`.\n\n### Suffix Array Substring Exact Deduplication\n\n```bash\n# input\npython -m text_dedup.suffix_array \\\n    --path "oscar-corpus/OSCAR-2201" \\\n    --name "gl" \\\n    --split "train" \\\n    --cache_dir "./cache" \\\n    --output "output/suffix_array/oscar_gl_dedup" \\\n    --column "text" \\\n    --google_repo_path "/Users/chenghao/Downloads/Projects/text-dedup/deduplicate-text-datasets"\n\n# output\nINFO     Loading                       : 2.75 seconds\nINFO     Preprocessing                 : 4.78 seconds\nINFO     SuffixArray                   : 98.29 seconds\nINFO     SelfSimilar                   : 4.24 seconds\nINFO     Restore                       : 0.25 seconds\nINFO     Deduplicate                   : 6.23 seconds\nINFO     Saving                        : 8.91 seconds\nINFO     Total                         : 125.45 seconds\nINFO     Before                        : 180332342 bytes (88803)\nINFO     After                         : 97646271 bytes (40404)\n```\n\n### MinHash Near Deduplication\n\n```bash\n# input\npython -m text_dedup.minhash \\\n  --path "oscar-corpus/OSCAR-2201" \\\n  --name "gl" \\\n  --split "train" \\\n  --cache_dir "./cache" \\\n  --output "output/minhash/oscar_gl_dedup" \\\n  --column "text" \\\n  --batch_size 10000\n\n# output\nINFO     Loading                         : 2.62 seconds\nINFO     MinHashing                      : 0.08 seconds\nINFO     Clustering                      : 2.20 seconds\nINFO     Filtering                       : 0.53 seconds\nINFO     Saving                          : 9.86 seconds\nINFO     Total                           : 15.29 seconds\nINFO     Data Number (before)            : 88803\nINFO     Data Number (after)             : 44124 (49.69%)\nINFO     Duplicate Number                : 44679 (50.31%)\nINFO     ðŸ¤— Happy Deduplicating ðŸ¤—\n```\n\n### SimHash Near Deduplication\n\n```bash\n# input\npython -m text_dedup.simhash \\\n  --path "oscar-corpus/OSCAR-2201" \\\n  --name "gl" \\\n  --split "train" \\\n  --cache_dir "./cache" \\\n  --output "output/simhash/oscar_gl_dedup" \\\n  --column "text" \\\n  --batch_size 10000\n\n# output\nINFO     Loading                         : 2.60 seconds\nINFO     SimHashing                      : 0.04 seconds\nINFO     Indexing                        : 28.88 seconds\nINFO     Filtering                       : 0.88 seconds\nINFO     Saving                          : 10.41 seconds\nINFO     Total                           : 42.80 seconds\nINFO     Data Number (before)            : 88803\nINFO     Data Number (after)             : 46163 (51.98%)\nINFO     Duplicate Number                : 42640 (48.02%)\nINFO     ðŸ¤— Happy Deduplicating ðŸ¤—\n```\n\n### Exact Hash Exact Deduplication\n\n```bash\n# input\npython -m text_dedup.exact_hash \\\n    --path "oscar-corpus/OSCAR-2201" \\\n    --name "gl" \\\n    --split "train" \\\n    --cache_dir "./cache" \\\n    --output "output/exact_hash/oscar_gl_dedup" \\\n    --column "text" \\\n    --batch_size 1000\n\n# output\nINFO     Loading                       : 2.95s\nINFO     Processing                    : 3.79s\nINFO     Filtering                     : 0.10s\nINFO     Saving                        : 2.89s\nINFO     Total                         : 9.72s\nINFO     Before                        : 88803\nINFO     After                         : 47049\n```\n\n### Bloom Filter Exact Deduplication\n\n```bash\n# input\npython -m text_dedup.bloom_filter \\\n    --path "oscar-corpus/OSCAR-2201" \\\n    --name "gl" \\\n    --split "train" \\\n    --cache_dir "./cache" \\\n    --output "output/bloom_filter/oscar_gl_dedup" \\\n    --error_rate 1e-5 \\\n    --column "text" \\\n    --batch_size 1000\n\n# output\nINFO     Loading                       : 2.72s\nINFO     Processing                    : 4.84s\nINFO     Filtering                     : 0.10s\nINFO     Saving                        : 2.88s\nINFO     Total                         : 10.54s\nINFO     Before                        : 88803\nINFO     After                         : 47045\n```\n\n## Benchmarks\n\nA benchmark of different methods here can be found in `benchmarks/wiki40.ipynb`. A notebook in evaluating MinHash on `pinecone/core-2020-05-10-deduplication` can be found in `benchmarks/pinecone.ipynb`.\n\nFor quick reference, here are the results:\n\n| Method                                                                          | Precision        | Recall           | F1               | Time |\n| ------------------------------------------------------------------------------- | ---------------- | ---------------- | ---------------- | ---- |\n| MinHash                                                                         | **0.9464** | **0.9446** | **0.9455** | 24s  |\n| SimHash\\*                                                                       | 0.9011           | 0.6959           | 0.7853           | 210s |\n| SimHash[(Gyawali et al., LREC 2020)](https://aclanthology.org/2020.lrec-1.113)     | 0.697            | 0.247            | 0.3647           | -    |\n| Exact Title (my implementation)                                                 | 0.8302           | 0.5521           | 0.6632           | -    |\n| Exact Title[(Gyawali et al., LREC 2020)](https://aclanthology.org/2020.lrec-1.113) | 0.830            | 0.50             | 0.624            | -    |\n\n\\*Best SimHash result from `benchmarks/hyperparameter.ipynb`.\n\n<!-- ## FAQ\n\n### Why use scripts instead of OOD classes and functions?\n\nEarly versions of the code uses object-oriented design for hashing and indexing, which was very difficult because different methods share little to no abstraction. In order to complie something that is useful, a lot of the wrapper code was used, and that actually increased the overhead of using this library. Additionally, deduplicating is often a one-time thing in data preprocessing pipeline, there isn\'t really a need for inline access. -->\n\n<!-- ### Why license change?\n\nBecause the google repo is licensed under Apache 2.0, I have to update from MIT. Util that part of code is completely re-implemented, Apache 2.0. will be the license I use. -->\n\n## License\n\n[Apache 2.0](https://duckduckgo.com/l/?uddg=https%3A%2F%2Fwww.apache.org%2Flicenses%2FLICENSE%2D2.0.html&rut=617d395c7a807de85e5707aca1f765e5b69a1627ed84c0aefa950e54e00a3094)\n',
    'author': 'Chenghao Mou',
    'author_email': 'mouchenghao@gmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.9,<3.12',
}


setup(**setup_kwargs)
